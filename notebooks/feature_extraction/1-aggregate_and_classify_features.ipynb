{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a12fde-fc04-47cd-85e3-685334bc1af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdc8443f-809b-4456-9ceb-b2a5d9025e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_type = \"plant\"\n",
    "\n",
    "features_of_interest = {\"plant\": [\"partner.diversity\", \"d\", \"normalised.degree\", \"weighted.betweenness\", \"weighted.closeness\"],\n",
    "                        \"network\": [\"connectance\", \"NODF\", \"modularity\", \"robustness\", \"robustness_mean\", \"robustness.LL\"]}\n",
    "\n",
    "features_dir = f\"../../data/features/{features_type}/\"\n",
    "networks_dir = f\"../../data/networks/all/\"\n",
    "network_types = [\"weighted\", \"binarized_weighted\", \"binary\"]\n",
    "\n",
    "plant_classification_path = f\"../../data/ploidy_classification/plant_classification.csv\"\n",
    "network_classification_path = f\"../../data/ploidy_classification/network_classification.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcff88a3-1f7d-4731-9a4c-34c198a26607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# output_paths = 0\n",
      "# result paths = 710\n",
      "# unsubmitted jobs = 192\n"
     ]
    }
   ],
   "source": [
    "output_paths = []\n",
    "features_paths = []\n",
    "to_submit = []\n",
    "for nt in network_types:\n",
    "    outdir = f\"{features_dir}{nt}/jobs_output/\"\n",
    "    resdir = f\"{features_dir}{nt}/features_by_network/\"\n",
    "    jobsdir = f\"{features_dir}{nt}/jobs/\"\n",
    "    unsubmitted = set([p.replace(\".sh\",\"\") for p in os.listdir(jobsdir)])-set([p.replace(\"_features.csv\",\"\") for p in os.listdir(resdir) if \"null\" not in p])\n",
    "    to_submit += [f\"{jobsdir}{j}.sh\" for j in unsubmitted]\n",
    "    l1 = [f\"{outdir}{p}\" for p in os.listdir(outdir) if p.endswith(\".out\")]\n",
    "    l2 = [f\"{resdir}{p}\" for p in os.listdir(resdir) if p.endswith(\".csv\") and \"null\" not in p]\n",
    "    output_paths += l1\n",
    "    features_paths += l2\n",
    "print(f\"# output_paths = {len(output_paths):,}\\n# result paths = {len(features_paths):,}\")\n",
    "print(f\"# unsubmitted jobs = {len(to_submit):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c46555f9-fb8a-450c-a399-9fa1b45201c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from subprocess import *\n",
    "from collections import defaultdict\n",
    "\n",
    "job_path_regex = re.compile(\"Submit_arguments\\s=.*?(\\/.*?)\\s\")\n",
    "\n",
    "jobs_log = str(Popen([\"qstat\", \"-u\", \"halabikeren\"], stdout=PIPE).communicate()[0]).split(\"\\\\n\")[5:]\n",
    "jobs_ids = [item.split(\".\")[0] for item in jobs_log if len(item.split(\".\")[0]) > 1]\n",
    "job_path_to_id = defaultdict(list)\n",
    "for job_id in jobs_ids:\n",
    "    try:\n",
    "        job_log = str(Popen([\"qstat\", \"-f\", job_id], stdout=PIPE).communicate()[0]).replace(\"\\\\n\",\"\").replace(\"\\\\t\",\"\")\n",
    "        job_path = job_path_regex.search(job_log).group(1)\n",
    "        job_path_to_id[job_path].append(job_id)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00fe7608-5545-437a-b98f-e09a4fb5e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for jp in to_submit:\n",
    "    if jp not in job_path_to_id:\n",
    "        print(jp)\n",
    "        # res=os.system(f\"qsub -q itaym {jp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "313d05b5-cca2-48bc-9aa2-fd3a99f6de47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# failed by unknown reason = 0\n",
      "# failed by memory = 0\n",
      "# failed by size = 0\n",
      "# jobs_to_rerun = 0\n"
     ]
    }
   ],
   "source": [
    "failed = []\n",
    "failed_mem = []\n",
    "failed_too_small = []\n",
    "to_rerun = []\n",
    "\n",
    "net_path_re = re.compile(\"network  (.*?) does not exist\")\n",
    "\n",
    "for p in output_paths:\n",
    "    res_path = p.replace(\"jobs_output\", \"features_by_network\").replace(\".out\", \"_features.csv\")\n",
    "    job_path = p.replace(\"jobs_output\", \"jobs\").replace(\".out\", \".sh\")\n",
    "    \n",
    "    if job_path in job_path_to_id:\n",
    "        continue\n",
    "\n",
    "    with open(p, \"r\") as f:\n",
    "        c=f.read()\n",
    "    if \"duration\" not in c and not os.path.exists(res_path):\n",
    "        failed.append(p)\n",
    "    if \"PBS: job killed: mem\" in c:\n",
    "        failed_mem.append(p)\n",
    "    if \"too small\" in c:\n",
    "        failed_too_small.append(p)\n",
    "    if \"does not exist\" in c:\n",
    "        net_path = net_path_re.search(c).group(1).replace(\"null\", \"all\").replace(\"/NA\", \".csv\")\n",
    "        net = pd.read_csv(net_path)\n",
    "        if net.shape[0] < 3 or net.shape[1] < 3:\n",
    "            failed_too_small.append(p)\n",
    "        else:\n",
    "            to_rerun.append(p)\n",
    "failed_other = set(failed)-set(failed_mem)-set(failed_too_small)-set(to_rerun)       \n",
    "print(f\"# failed by unknown reason = {len(failed_other):,}\\n# failed by memory = {len(failed_mem):,}\\n# failed by size = {len(failed_too_small):,}\\n# jobs_to_rerun = {len(to_rerun)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e20cf355-0056-4134-bbea-b2ac5772bcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/groups/itay_mayrose/halabikeren/plant_pollinator_networks/notebooks/feature_extraction\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "jobs=[]\n",
    "for j in to_rerun:\n",
    "    jobs.append(j.replace(\".out\",\".sh\").replace(\"_output\",\"\"))\n",
    "print(os.getcwd())\n",
    "print(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ff0e59b-ebdf-42c6-aeaf-8510e162cd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# missing output = 18\n"
     ]
    }
   ],
   "source": [
    "missing = {}\n",
    "nmissing = 0\n",
    "for nt in network_types:\n",
    "    networks = [p.replace(\".csv\", \"\") for p in os.listdir(f\"{networks_dir}{nt}/\") if p.endswith(\".csv\")]\n",
    "    results_dir = f\"{features_dir}{nt}/features_by_network/\"\n",
    "    resulting_networks = [p.replace(\"_features.csv\",\"\") for p in os.listdir(results_dir) if \"null\" not in p and p.endswith(\".csv\")]\n",
    "    missing[nt] = set(networks)-set(resulting_networks)\n",
    "    nmissing += len(missing[nt])\n",
    "print(f\"# missing output = {nmissing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cebc48d4-82d7-4fc6-ab12-24f71981ea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# jobs for binary = 3\n",
      "['../../data/features/network/binary/jobs/155.sh', '../../data/features/network/binary/jobs/96.sh', '../../data/features/network/binary/jobs/0.sh']\n",
      "\n",
      "\n",
      "# jobs for weighted = 15\n",
      "['../../data/features/network/weighted/jobs/104.sh', '../../data/features/network/weighted/jobs/73.sh', '../../data/features/network/weighted/jobs/72.sh', '../../data/features/network/weighted/jobs/2.sh', '../../data/features/network/weighted/jobs/383.sh', '../../data/features/network/weighted/jobs/79.sh', '../../data/features/network/weighted/jobs/388.sh', '../../data/features/network/weighted/jobs/483.sh', '../../data/features/network/weighted/jobs/260.sh', '../../data/features/network/weighted/jobs/231.sh', '../../data/features/network/weighted/jobs/387.sh', '../../data/features/network/weighted/jobs/269.sh', '../../data/features/network/weighted/jobs/4.sh', '../../data/features/network/weighted/jobs/357.sh', '../../data/features/network/weighted/jobs/386.sh']\n",
      "\n",
      "\n",
      "# jobs for binarized_weighted = 12\n",
      "['../../data/features/network/binarized_weighted/jobs/455.sh', '../../data/features/network/binarized_weighted/jobs/136.sh', '../../data/features/network/binarized_weighted/jobs/343.sh', '../../data/features/network/binarized_weighted/jobs/351.sh', '../../data/features/network/binarized_weighted/jobs/201.sh', '../../data/features/network/binarized_weighted/jobs/144.sh', '../../data/features/network/binarized_weighted/jobs/441.sh', '../../data/features/network/binarized_weighted/jobs/466.sh', '../../data/features/network/binarized_weighted/jobs/316.sh', '../../data/features/network/binarized_weighted/jobs/536.sh', '../../data/features/network/binarized_weighted/jobs/468.sh', '../../data/features/network/binarized_weighted/jobs/154.sh']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "failed_mem_jobs = [p.replace(\"_output\", \"\").replace(\".out\", \".sh\") for p in failed_mem]\n",
    "for nt in network_types:\n",
    "    net_id_to_job = {}\n",
    "    jobs_dir = f\"../../data/features/{features_type}/{nt}/jobs/\"\n",
    "    job_paths = [f\"{jobs_dir}/{path}\" for path in os.listdir(jobs_dir) if path.endswith(\".sh\")]\n",
    "    net_id_regex = re.compile(\"(\\d*)_features.csv\", re.MULTILINE)\n",
    "    for path in job_paths:\n",
    "        try:\n",
    "            job_id = int(os.path.basename(path).replace(\".sh\",\"\"))\n",
    "            with open(path, \"r\") as f:\n",
    "                net_id = int(net_id_regex.search(f.read()).group(1))\n",
    "            net_id_to_job[net_id] = job_id \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    jobs = []\n",
    "    for net_id in missing[nt]:\n",
    "        job_id = net_id_to_job[int(net_id)]\n",
    "        job_path = f\"{jobs_dir}{job_id}.sh\"\n",
    "        if job_path not in failed_mem_jobs:\n",
    "            jobs.append(job_path)\n",
    "    print(f\"# jobs for {nt} = {len(jobs)}\")\n",
    "    print(jobs)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "49843a84-e093-4b50-8fc8-8c2f5e0bd122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nt in missing:\n",
    "#     for m in missing[nt]:\n",
    "#         jp = f\"{features_dir}{nt}/jobs/{m}.sh\"\n",
    "#         with open(jp, \"r\") as f:\n",
    "#             c=f.read()\n",
    "#         c=c.replace(\"/_features/\", \"/features/\").replace(\"/features/_by_network/\", \"/features_by_network/\")\n",
    "#         with open(jp, \"w\") as f:\n",
    "#             f.write(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "338f1cab-394c-4a12-ba58-b36bb15cc256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nt in missing:\n",
    "#     for m in missing[nt]:\n",
    "#         jp = f\"{features_dir}{nt}/jobs/{m}.sh\"\n",
    "#         res = os.system(f\"qsub -q itay_75 {jp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d2a7961-051f-4c39-a65e-2802d2ec6173",
   "metadata": {},
   "outputs": [],
   "source": [
    "for jop in failed_mem:\n",
    "    jp = jop.replace(\"jobs_output\", \"jobs\").replace(\".out\", \".sh\")\n",
    "    with open(jp, \"r\") as f:\n",
    "        c=f.read()\n",
    "    c=c.replace(\"/features\", \"/_features/\").replace(\"15gb\", \"30gb\").replace(\"50gb\", \"80gb\").replace(\"30gb\", \"50gb\").replace(\"20gb\", \"30gb\").replace(\"15gb\", \"20gb\").replace(\"10gb\", \"15gb\").replace(\"4gb\", \"10gb\")\n",
    "    with open(jp, \"w\") as f:\n",
    "        f.write(c)\n",
    "    if os.path.exists(jop):\n",
    "        os.remove(jop)\n",
    "    # res=os.system(f\"qsub -q itay_75 {jp}\")\n",
    "    print(jp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7868b4b7-c57e-415c-a19a-7da986ad60a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in to_rerun:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "    jp = f.replace(\"jobs_output\", \"jobs\").replace(\".out\", \".sh\")\n",
    "    res = os.system(f\"qsub -q itay_75 {jp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92b2cb30-d385-4bfe-82b4-82df0d5924cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# weighted networks for analysis = 710\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "for p in features_paths:\n",
    "    nt = p.split(\"/\")[-3]\n",
    "    df = pd.read_csv(p)\n",
    "    sd_cols = [col for col in df.columns if col.startswith(\"standardized_\")]\n",
    "    for c in sd_cols:\n",
    "        df[c] = df[c].apply(lambda x: np.nan if x < -10000 or x > 10000 else x)\n",
    "    df[\"network_type\"] = nt\n",
    "    features.append(df)\n",
    "features = pd.concat(features)\n",
    "if features_type == \"plant\":\n",
    "    if \"Plant\" not in features.columns:\n",
    "        features = features.rename(columns={\"Unnamed: 0\": \"Plant\"})\n",
    "    features.Plant = features.Plant.str.lower()\n",
    "if \"network_id\" in features.columns:\n",
    "    if features[\"network\"].dtype == str:\n",
    "        features[\"network_id\"] = features[\"network\"].str.replace(\".csv\",\"\").astype(int)\n",
    "    else:\n",
    "        features[\"network_id\"] = features[\"network\"] \n",
    "features.to_csv(f\"{features_dir}/all_features.csv\")\n",
    "for nt in network_types:\n",
    "    nt_features = features.loc[features.network_type == nt]\n",
    "    print(f\"# {nt} networks for analysis = {len(nt_features.network.unique()):,}\")\n",
    "    nt_features.to_csv(f\"{features_dir}/{nt}/features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eab496-098b-4bad-9e1f-cb302244e8f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# add classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eec1c4c-c077-480d-b912-140dd0b4541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df = pd.read_csv(plant_classification_path if features_type == \"plant\" else network_classification_path)\n",
    "classification_merge_cols = [\"network_id\"] if features_type == \"network\" else [\"original_name\"]\n",
    "features_merge_cols = [\"network_id\"] if features_type == \"network\" else [\"Plant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f79e0ef-0a0e-48c0-b87c-e78e8ee8c6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.211755.power9.tau.ac.il/ipykernel_316129/1933274399.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  features[\"network_id\"] = features.network.str.replace(\".csv\",\"\").astype(int)\n"
     ]
    }
   ],
   "source": [
    "features[\"network_id\"] = features.network.str.replace(\".csv\",\"\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b44467bd-edb5-4ce7-bcb2-57ce49336d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.211755.power9.tau.ac.il/ipykernel_316129/1085764363.py:10: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  print(df[set(classification_df.columns)&set(df.columns)].notna().sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched_name                 5636\n",
      "is_polyploid_by_matched         0\n",
      "is_polyploid                 5755\n",
      "polyploid_age                5754\n",
      "is_polyploid_by_original     5490\n",
      "polyploid_age_by_matched        0\n",
      "is_polyploid_by_resolved     5604\n",
      "polyploid_age_by_original    5489\n",
      "original_name                5755\n",
      "resolved_name                5636\n",
      "polyploid_age_by_resolved    5603\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for nt in network_types:\n",
    "    df = features.loc[features.network_type == nt]\n",
    "    relevant_classification_data = classification_df\n",
    "    if \"network_type\" in classification_df.columns:\n",
    "        relevant_classification_data = classification_df.query(f\"network_type == '{nt}'\").drop([\"network_type\"], axis=1)\n",
    "    df = df.merge(relevant_classification_data, \n",
    "                  left_on=features_merge_cols,\n",
    "                  right_on=classification_merge_cols,\n",
    "                  how=\"left\")\n",
    "    print(df[set(classification_df.columns)&set(df.columns)].notna().sum())\n",
    "    df.to_csv(f\"{features_dir}/{nt}/features_with_classification.csv\", index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5742e68f-66e6-494c-aa31-99506aa3cc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resolved_name                0.557413\n",
       "matched_name                 0.557413\n",
       "original_name                0.569182\n",
       "is_polyploid_by_original     0.542973\n",
       "polyploid_age_by_original    0.542874\n",
       "is_polyploid_by_matched      0.000000\n",
       "polyploid_age_by_matched     0.000000\n",
       "is_polyploid_by_resolved     0.554248\n",
       "polyploid_age_by_resolved    0.554149\n",
       "is_polyploid                 0.569182\n",
       "polyploid_age                0.569083\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for nt in network_types:\n",
    "    df = pd.read_csv(f\"{features_dir}/{nt}/features_with_classification.csv\")\n",
    "    df = df[[c for c in df.columns if \"Unnamed\" not in c and not \"standardized_\" in c]]\n",
    "    df = df.rename(columns={c: c.replace(\"_y\",\"\") for c in df.columns if c.endswith(\"_y\")})\n",
    "    df = df[[c for c in df.columns if not c.endswith(\"_x\")]]\n",
    "    display(df[[c for c in relevant_classification_data.columns if c in df.columns]].notna().sum() / df.shape[0])\n",
    "    df.to_csv(f\"{features_dir}/{nt}/features_with_classification.csv\", index=False)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
